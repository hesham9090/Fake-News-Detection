{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Politifact_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfktFh9oLx19",
        "colab_type": "code",
        "outputId": "f73f9b4f-6c7b-49f0-89a0-aa67820a93f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZZFV8KoQbL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import re\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkbAhdeZXX3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Preprocess_text(data):\n",
        "    data['Title'] = data['Title'].apply(lambda x: re.sub(r'[^a-zA-Z0-9 ]',r' ',x))\n",
        "    data['Title'] = data['Title'].apply(lambda x: x.lower())\n",
        "    data['Title'] = data['Title'].apply(lambda x: re.sub(r'[0-9]+',r' NUM ',x))\n",
        "    data['Article'] = data['Article'].apply(lambda x: re.sub(r'[^a-zA-Z0-9 ]',r' ',x))\n",
        "    data['Article'] = data['Article'].apply(lambda x: x.lower())\n",
        "    data['Article'] = data['Article'].apply(lambda x: re.sub(r'[0-9]+',r' NUM ',x))\n",
        "    data['Author'] = data['Author'].apply(lambda x: re.sub(r'[^a-zA-Z0-9 ]',r' ',x))\n",
        "    data['Author'] = data['Author'].apply(lambda x: x.lower())\n",
        "    data['Author'] = data['Author'].apply(lambda x: re.sub(r'[0-9]+',r' NUM ',x))\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2-Xq2gyLq2I",
        "colab_type": "code",
        "outputId": "2b6e3d6a-7480-416d-bc45-3f7f7248c790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Fake-News-Detection/dataset/Poltifact_dataset.csv',sep=',',index_col='idx')\n",
        "df = df.loc[(df.Labels=='Pants on Fire!') | (df.Labels=='TRUE')] # for two labels\n",
        "df = Preprocess_text(df)\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "#df['Image'] = labelencoder.inverse_transform(df['Image'])\n",
        "df['Labels'] = labelencoder.fit_transform(df['Labels'])\n",
        "print(labelencoder.classes_)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Pants on Fire!' 'TRUE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Date</th>\n",
              "      <th>Author</th>\n",
              "      <th>Article</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>idx</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>no  bill cosby wasn t framed for sexual assaul...</td>\n",
              "      <td>on Monday, November 4th, 2019 at 2:31 p.m.</td>\n",
              "      <td>samantha putterman</td>\n",
              "      <td>actor and comedian bill cosby  was convicted...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kamala harris didn t say democrats are  coming...</td>\n",
              "      <td>on Monday, November 4th, 2019 at 1:03 p.m.</td>\n",
              "      <td>daniel funke</td>\n",
              "      <td>a quote from sen  kamala harris attacking su...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>no evidence that cream of tartar and orange ju...</td>\n",
              "      <td>on Wednesday, October 30th, 2019 at 2:03 p.m.</td>\n",
              "      <td>samantha putterman</td>\n",
              "      <td>quitting smoking is hard  over the years  ex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>cnn did not call abu bakr al baghdadi an  unar...</td>\n",
              "      <td>on Wednesday, October 30th, 2019 at 1:07 p.m.</td>\n",
              "      <td>daniel funke</td>\n",
              "      <td>a viral photo of a cnn chyron makes it look ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>fact check  does california really have   NUM ...</td>\n",
              "      <td>on Thursday, October 31st, 2019 at 11:41 a.m.</td>\n",
              "      <td>chris nichols</td>\n",
              "      <td>california s homeless crisis now ranks as on...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Title  ... Labels\n",
              "idx                                                     ...       \n",
              "0    no  bill cosby wasn t framed for sexual assaul...  ...      0\n",
              "3    kamala harris didn t say democrats are  coming...  ...      0\n",
              "10   no evidence that cream of tartar and orange ju...  ...      0\n",
              "11   cnn did not call abu bakr al baghdadi an  unar...  ...      0\n",
              "14   fact check  does california really have   NUM ...  ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZp-_pSCLq2R",
        "colab_type": "code",
        "outputId": "e5674d65-68d6-4a6d-b2af-b4bf2b4d4761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "train_all = df[['Author','Title','Article']]\n",
        "target_all = df['Labels']\n",
        "column = df['Title']\n",
        "print(column[3])\n",
        "train_all.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kamala harris didn t say democrats are  coming for  trump supporters in  NUM \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author</th>\n",
              "      <th>Title</th>\n",
              "      <th>Article</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>idx</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>samantha putterman</td>\n",
              "      <td>no  bill cosby wasn t framed for sexual assaul...</td>\n",
              "      <td>actor and comedian bill cosby  was convicted...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>daniel funke</td>\n",
              "      <td>kamala harris didn t say democrats are  coming...</td>\n",
              "      <td>a quote from sen  kamala harris attacking su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>samantha putterman</td>\n",
              "      <td>no evidence that cream of tartar and orange ju...</td>\n",
              "      <td>quitting smoking is hard  over the years  ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>daniel funke</td>\n",
              "      <td>cnn did not call abu bakr al baghdadi an  unar...</td>\n",
              "      <td>a viral photo of a cnn chyron makes it look ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>chris nichols</td>\n",
              "      <td>fact check  does california really have   NUM ...</td>\n",
              "      <td>california s homeless crisis now ranks as on...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Author  ...                                            Article\n",
              "idx                      ...                                                   \n",
              "0    samantha putterman  ...    actor and comedian bill cosby  was convicted...\n",
              "3          daniel funke  ...    a quote from sen  kamala harris attacking su...\n",
              "10   samantha putterman  ...    quitting smoking is hard  over the years  ex...\n",
              "11         daniel funke  ...    a viral photo of a cnn chyron makes it look ...\n",
              "14        chris nichols  ...    california s homeless crisis now ranks as on...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwZdNSYeLq2V",
        "colab_type": "code",
        "outputId": "6675db43-eb5e-463e-b6ea-abc6564c9df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "max_length = max([len(s.split()) for s in train_all['Article']])\n",
        "max_length2 = max([len(s.split()) for s in train_all['Title']])\n",
        "max_length3 = max([len(s.split()) for s in train_all['Author']])\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_all,target_all, test_size=0.2, random_state=42)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer2 = Tokenizer()\n",
        "tokenizer3 = Tokenizer()\n",
        "tokenizer.fit_on_texts(list(train_all['Article']))\n",
        "x_train_Article_tokens = tokenizer.texts_to_sequences(list(x_train['Article']))\n",
        "x_test_Article_tokens = tokenizer.texts_to_sequences(list(x_test['Article']))\n",
        "x_train_Article_pad = pad_sequences(x_train_Article_tokens,maxlen=max_length,padding='post')\n",
        "x_test_Article_pad = pad_sequences(x_test_Article_tokens,maxlen=max_length,padding='post')\n",
        "\n",
        "unique_words = len(tokenizer.word_index)\n",
        "\n",
        "tokenizer2.fit_on_texts(train_all['Title'])\n",
        "x_train_Title_tokens = tokenizer2.texts_to_sequences(x_train['Title'])\n",
        "x_test_Title_tokens = tokenizer2.texts_to_sequences(x_test['Title'])\n",
        "x_train_Title_pad = pad_sequences(x_train_Title_tokens,maxlen=max_length2,padding='post')\n",
        "x_test_Title_pad = pad_sequences(x_test_Title_tokens,maxlen=max_length2,padding='post')\n",
        "\n",
        "unique_words = len(tokenizer.word_index)+1\n",
        "\n",
        "tokenizer3.fit_on_texts(train_all['Author'])\n",
        "x_train_Author_tokens = tokenizer3.texts_to_sequences(x_train['Author'])\n",
        "x_test_Author_tokens = tokenizer3.texts_to_sequences(x_test['Author'])\n",
        "x_train_Author_pad = pad_sequences(x_train_Author_tokens,maxlen=max_length3,padding='post')\n",
        "x_test_Author_pad = pad_sequences(x_test_Author_tokens,maxlen=max_length3,padding='post')\n",
        "\n",
        "unique_words = len(tokenizer.word_index)+1\n",
        "\n",
        "print(x_train_Article_pad.shape)\n",
        "print(x_train_Title_pad.shape)\n",
        "print(x_train_Author_pad.shape)\n",
        "print(x_test_Article_pad.shape)\n",
        "print(x_test_Title_pad.shape)\n",
        "print(x_test_Author_pad.shape)\n",
        "print('Found %d unique words.' % len(tokenizer.word_index))\n",
        "print('Found %d unique words.' % len(tokenizer2.word_index))\n",
        "print('Found %d unique words.' % len(tokenizer3.word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3328, 3229)\n",
            "(3328, 31)\n",
            "(3328, 4)\n",
            "(832, 3229)\n",
            "(832, 31)\n",
            "(832, 4)\n",
            "Found 42697 unique words.\n",
            "Found 7582 unique words.\n",
            "Found 479 unique words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzujHgnLLq2Z",
        "colab_type": "code",
        "outputId": "4137873f-6a69-4436-f789-58611b1c5861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "#GLOVE_DIR = 'glove.6B/'\n",
        "f = open('/content/drive/My Drive/Fake-News-Detection/glove.6B.50d.txt',encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85nruGpvLq2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "word_index2 = tokenizer2.word_index\n",
        "word_index3 = tokenizer3.word_index\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "embedding_matrix2 = np.zeros((len(word_index2) + 1, 50))\n",
        "for word, i in word_index2.items():\n",
        "    embedding_vector2 = embeddings_index.get(word)\n",
        "    if embedding_vector2 is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix2[i] = embedding_vector2\n",
        "embedding_matrix3 = np.zeros((len(word_index3) + 1, 50))\n",
        "for word, i in word_index3.items():\n",
        "    embedding_vector3 = embeddings_index.get(word)\n",
        "    if embedding_vector3 is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix3[i] = embedding_vector3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSM_mtXwBF5L",
        "colab_type": "code",
        "outputId": "1656f8e5-75c7-410f-8851-1cd5033baf5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense,Conv1D, GlobalMaxPooling1D, Dropout, Embedding, BatchNormalization, Conv2D,Reshape,Activation,Concatenate, Input,LSTM,Bidirectional, GRU,MaxPool2D,Flatten, Dropout, Dense, Conv1D\n",
        "from keras.layers import MaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "# the dimension of vectors to be used\n",
        "\n",
        "# filter sizes of the different conv layers \n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 512\n",
        "embedding_dim = 20\n",
        "# dropout probability\n",
        "drop = 0.4\n",
        "batch_size = 64\n",
        "epochs = 2\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "# # # # # # \n",
        "inputAr = Input(shape=max_length)\n",
        "input_Article = Embedding(len(word_index) +1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False,name='Article')(inputAr)\n",
        "reshape_1 = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(input_Article)\n",
        "hidden2 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='he_normal', activation='relu')(reshape_1)\n",
        "\n",
        "\n",
        "# # # # # # # # # # # # # \n",
        "inputT = Input(shape=max_length2)\n",
        "input_Title = Embedding(len(word_index2) +1,EMBEDDING_DIM,weights=[embedding_matrix2],trainable=False,name='Title')(inputT)\n",
        "reshape_2 = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(input_Title)\n",
        "hidden2_2 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='he_normal', activation='relu')(reshape_2)\n",
        "\n",
        "# # # # # # # # # # # # # \n",
        "inputAu = Input(shape=max_length3)\n",
        "input_Author  = Embedding(len(word_index3) +1, EMBEDDING_DIM,weights=[embedding_matrix3],trainable=False,name='Author')(inputT)\n",
        "reshape_3 = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(input_Author)\n",
        "hidden2_3 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='he_normal', activation='relu')(reshape_3)\n",
        "\n",
        "# # # # # # # # # # # # # \n",
        "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(hidden2)\n",
        "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(hidden2_2)\n",
        "maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(hidden2_3)\n",
        "\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "# # # # # # # # # # # # # \n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=20, activation='softmax')(dropout)\n",
        "#dropout = Dropout(0.4)(concatenated_tensor)\n",
        "#dense = Dense(200,kernel_initializer='he_normal')(dropout)\n",
        "#batch_norm2 = BatchNormalization()(dense)\n",
        "#activation = Activation('relu')(batch_norm2)\n",
        "#dense2 = Dense(50,kernel_initializer='he_normal')(activation)\n",
        "#batch_norm3 = BatchNormalization()(dense2)\n",
        "#activation2 = Activation('relu')(batch_norm3)\n",
        "#dropout2 = Dropout(0.4)(activation2)\n",
        "#output = Dense(10, activation='sigmoid')(dropout2)\n",
        "# # # # # # # # # # # # # \n",
        "\n",
        "# this creates a model that includes\n",
        "model = keras.Model(inputs=[inputAr,inputT,inputAu], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3229)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 31)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Article (Embedding)             (None, 3229, 50)     2134900     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Title (Embedding)               (None, 31, 50)       379150      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Author (Embedding)              (None, 31, 50)       24000       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 1000, 50, 1)  0           Article[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 1000, 50, 1)  0           Title[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1000, 50, 1)  0           Author[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 998, 31, 512) 31232       reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 998, 31, 512) 31232       reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 998, 31, 512) 31232       reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 1, 31, 512)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 2, 31, 512)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 3, 31, 512)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 6, 31, 512)   0           max_pooling2d[0][0]              \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 95232)        0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 95232)        0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 20)           1904660     dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 4,536,406\n",
            "Trainable params: 1,998,356\n",
            "Non-trainable params: 2,538,050\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHx1qzG3Lq2l",
        "colab_type": "code",
        "outputId": "d573c3ce-4bf4-4615-91f8-c1bc0e44378f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "source": [
        "import numpy as np\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history =model.fit((x_train_Article_pad,x_train_Title_pad,x_train_Author_pad), y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2662 samples, validate on 666 samples\n",
            "Epoch 1/2\n",
            "  64/2662 [..............................] - ETA: 38s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-56561d8877b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_Article_pad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_Title_pad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_Author_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  indices[56,5] = 1842 is not in [0, 480)\n\t [[node model/Author/embedding_lookup (defined at /tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py:1751) ]]\n\t [[GroupCrossDeviceControlEdges_0/ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3/_25]]\n  (1) Invalid argument:  indices[56,5] = 1842 is not in [0, 480)\n\t [[node model/Author/embedding_lookup (defined at /tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py:1751) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_22701]\n\nFunction call stack:\ndistributed_function -> distributed_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-jw-mriLq2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "#plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}