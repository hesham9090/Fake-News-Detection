{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FALSE' 'Half-True' 'Mostly False' 'Mostly True' 'Pants on Fire!' 'TRUE']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Article</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no bill cosby wasnt framed for sexual assault ...</td>\n",
       "      <td>on Monday, November 4th, 2019 at 2:31 p.m.</td>\n",
       "      <td>samantha putterman</td>\n",
       "      <td>actor and comedian bill cosby was convicted on...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>green new deal doesnt mention abortion but nik...</td>\n",
       "      <td>on Friday, November 1st, 2019 at 1:39 p.m.</td>\n",
       "      <td>tom kertscher</td>\n",
       "      <td>attacks on the green new deal include claims t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subpoenas vs bills what this cnn pundit got wrong</td>\n",
       "      <td>on Monday, November 4th, 2019 at 2:03 p.m.</td>\n",
       "      <td>bill mccarthy</td>\n",
       "      <td>cnn commentator mike shields took aim at house...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kamala harris didnt say democrats are coming f...</td>\n",
       "      <td>on Monday, November 4th, 2019 at 1:03 p.m.</td>\n",
       "      <td>daniel funke</td>\n",
       "      <td>a quote from sen kamala harris attacking suppo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nancy pelosi overstates republican gains in im...</td>\n",
       "      <td>on Monday, November 4th, 2019 at 11:44 a.m.</td>\n",
       "      <td>jon greenberg</td>\n",
       "      <td>house republicans voted unanimously against a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "idx                                                      \n",
       "0    no bill cosby wasnt framed for sexual assault ...   \n",
       "1    green new deal doesnt mention abortion but nik...   \n",
       "2    subpoenas vs bills what this cnn pundit got wrong   \n",
       "3    kamala harris didnt say democrats are coming f...   \n",
       "4    nancy pelosi overstates republican gains in im...   \n",
       "\n",
       "                                            Date              Author  \\\n",
       "idx                                                                    \n",
       "0     on Monday, November 4th, 2019 at 2:31 p.m.  samantha putterman   \n",
       "1     on Friday, November 1st, 2019 at 1:39 p.m.       tom kertscher   \n",
       "2     on Monday, November 4th, 2019 at 2:03 p.m.       bill mccarthy   \n",
       "3     on Monday, November 4th, 2019 at 1:03 p.m.        daniel funke   \n",
       "4    on Monday, November 4th, 2019 at 11:44 a.m.       jon greenberg   \n",
       "\n",
       "                                               Article  Labels  \n",
       "idx                                                             \n",
       "0    actor and comedian bill cosby was convicted on...       4  \n",
       "1    attacks on the green new deal include claims t...       2  \n",
       "2    cnn commentator mike shields took aim at house...       0  \n",
       "3    a quote from sen kamala harris attacking suppo...       4  \n",
       "4    house republicans voted unanimously against a ...       2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import re\n",
    "import Preprocessing\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df = pd.read_csv('Poltifact_dataset.csv',sep=',',index_col='idx')\n",
    "df = Preprocessing.Preprocess_text(df)\n",
    "#df['Image'] = labelencoder.inverse_transform(df['Image'])\n",
    "df['Labels'] = labelencoder.fit_transform(df['Labels'])\n",
    "print(labelencoder.classes_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kamala Harris didnt say Democrats are coming for Trump supporters in NUM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Bill Cosby wasnt framed for sexual assault ...</td>\n",
       "      <td>Actor and comedian Bill Cosby was convicted on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Green New Deal doesnt mention abortion but Nik...</td>\n",
       "      <td>Attacks on the Green New Deal include claims t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subpoenas vs bills What this CNN pundit got wrong</td>\n",
       "      <td>CNN commentator Mike Shields took aim at House...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kamala Harris didnt say Democrats are coming f...</td>\n",
       "      <td>A quote from Sen Kamala Harris attacking suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nancy Pelosi overstates Republican gains in im...</td>\n",
       "      <td>House Republicans voted unanimously against a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "idx                                                      \n",
       "0    No Bill Cosby wasnt framed for sexual assault ...   \n",
       "1    Green New Deal doesnt mention abortion but Nik...   \n",
       "2    Subpoenas vs bills What this CNN pundit got wrong   \n",
       "3    Kamala Harris didnt say Democrats are coming f...   \n",
       "4    Nancy Pelosi overstates Republican gains in im...   \n",
       "\n",
       "                                               Article  \n",
       "idx                                                     \n",
       "0    Actor and comedian Bill Cosby was convicted on...  \n",
       "1    Attacks on the Green New Deal include claims t...  \n",
       "2    CNN commentator Mike Shields took aim at House...  \n",
       "3    A quote from Sen Kamala Harris attacking suppo...  \n",
       "4    House Republicans voted unanimously against a ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all = df[['Author','Title','Article']]\n",
    "target_all = df['Labels']\n",
    "column = df['Title']\n",
    "print(column[3])\n",
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13092, 3107)\n",
      "(13092, 34)\n",
      "(3273, 3107)\n",
      "(3273, 34)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_length = max([len(s.split()) for s in train_all['Article']])\n",
    "max_length2 = max([len(s.split()) for s in train_all['Title']])\n",
    "max_length3 = max([len(s.split()) for s in train_all['Author']])\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_all,target_all, test_size=0.2, random_state=42)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer3 = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_all['Article']))\n",
    "x_train_Article_tokens = tokenizer.texts_to_sequences(list(x_train['Article']))\n",
    "x_test_Article_tokens = tokenizer.texts_to_sequences(list(x_test['Article']))\n",
    "x_train_Article_pad = pad_sequences(x_train_Article_tokens,maxlen=max_length,padding='post')\n",
    "x_test_Article_pad = pad_sequences(x_test_Article_tokens,maxlen=max_length,padding='post')\n",
    "\n",
    "unique_words = len(tokenizer.word_index)\n",
    "\n",
    "tokenizer2.fit_on_texts(train_all['Title'])\n",
    "x_train_Title_tokens = tokenizer2.texts_to_sequences(x_train['Title'])\n",
    "x_test_Title_tokens = tokenizer2.texts_to_sequences(x_test['Title'])\n",
    "x_train_Title_pad = pad_sequences(x_train_Title_tokens,maxlen=max_length2,padding='post')\n",
    "x_test_Title_pad = pad_sequences(x_test_Title_tokens,maxlen=max_length2,padding='post')\n",
    "\n",
    "unique_words = len(tokenizer.word_index)+1\n",
    "\n",
    "tokenizer3.fit_on_texts(train_all['Author'])\n",
    "x_train_Author_tokens = tokenizer3.texts_to_sequences(x_train['Author'])\n",
    "x_test_Author_tokens = tokenizer3.texts_to_sequences(x_test['Author'])\n",
    "x_train_Author_pad = pad_sequences(x_train_Author_tokens,maxlen=max_length3,padding='post')\n",
    "x_test_Author_pad = pad_sequences(x_test_Author_tokens,maxlen=max_length3,padding='post')\n",
    "\n",
    "unique_words = len(tokenizer.word_index)+1\n",
    "\n",
    "print(x_train_Article_pad.shape)\n",
    "print(x_train_Title_pad.shape)\n",
    "print(x_train_Author_pad.shape)\n",
    "print(x_test_Article_pad.shape)\n",
    "print(x_test_Title_pad.shape)\n",
    "print(x_test_Author_pad.shape)\n",
    "print('Found %d unique words.' % len(tokenizer.word_index))\n",
    "print('Found %d unique words.' % len(tokenizer2.word_index))\n",
    "print('Found %d unique words.' % len(tokenizer3.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "#GLOVE_DIR = 'glove.6B/'\n",
    "f = open('/content/drive/My Drive/glove.6B.50d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_index2 = tokenizer2.word_index\n",
    "word_index3 = tokenizer3.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix2 = np.zeros((len(word_index2) + 1, 50))\n",
    "for word, i in word_index2.items():\n",
    "    embedding_vector2 = embeddings_index.get(word)\n",
    "    if embedding_vector2 is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix2[i] = embedding_vector2\n",
    "embedding_matrix3 = np.zeros((len(word_index3) + 1, 50))\n",
    "for word, i in word_index3.items():\n",
    "    embedding_vector3 = embeddings_index.get(word)\n",
    "    if embedding_vector3 is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix3[i] = embedding_vector3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, Embedding, BatchNormalization, Activation,Concatenate,Input,LSTM,Bidirectional\n",
    "\n",
    "#y_train = y_train.to_numpy()\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "#with tf.device('/cpu:0'):\n",
    "#model = keras.models.Sequential()\n",
    "inputAr = Input(shape=max_length)\n",
    "input_Article = Embedding(len(word_index) + 1,50,weights=[embedding_matrix],\n",
    "                          trainable=False,name='Article')(inputAr)\n",
    "#hidden1 = LSTM(100,return_sequences=True)(input_Article)\n",
    "hidden2 = Bidirectional(LSTM(231,dropout=0.1,recurrent_dropout=0.1))(input_Article)\n",
    "dense3 = Dense(200,kernel_initializer='he_normal')(hidden2)\n",
    "\n",
    "inputT = Input(shape=max_length2)\n",
    "input_Title = Embedding(len(word_index2) + 1,50,weights=[embedding_matrix2],\n",
    "                        trainable=False,name='Title')(inputT)\n",
    "\n",
    "#hidden1_2 = LSTM(20,return_sequences=True)(input_Title)\n",
    "hidden2_2 = Bidirectional(LSTM(46,dropout=0.1,recurrent_dropout=0.1))(input_Title)\n",
    "dense3_2 = Dense(50,kernel_initializer='he_normal')(hidden2_2)\n",
    "\n",
    "inputAu = Input(shape=max_length3)\n",
    "input_Author = Embedding(len(word_index3) + 1,50,weights=[embedding_matrix3],\n",
    "                          trainable=False,name='Author')(inputAu)\n",
    "#hidden1_3 = LSTM(5,return_sequences=True)(input_Author)\n",
    "hidden2_3 = Bidirectional(LSTM(10,dropout=0.1,recurrent_dropout=0.1))(input_Author)\n",
    "dense3_3 = Dense(20,kernel_initializer='he_normal')(hidden2_3)\n",
    "\n",
    "concat = keras.layers.concatenate([dense3,dense3_2,dense3_3])\n",
    "\n",
    "#batch_norm = BatchNormalization()(concat)\n",
    "dropout = Dropout(0.6)(concat)\n",
    "dense = Dense(200,kernel_initializer='he_normal')(dropout)\n",
    "batch_norm2 = BatchNormalization()(dense)\n",
    "activation = Activation('relu')(batch_norm2)\n",
    "dense2 = Dense(50,kernel_initializer='he_normal')(activation)\n",
    "batch_norm3 = BatchNormalization()(dense2)\n",
    "activation2 = Activation('relu')(batch_norm3)\n",
    "dropout2 = Dropout(0.6)(activation2)\n",
    "output = Dense(2, activation='softmax')(dropout2)\n",
    "model = keras.Model(inputs=[inputAr, inputT, inputAu], outputs=[output])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='RMSprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit((x_train_Article_pad, x_train_Title_pad, x_train_Author_pad), y_train, epochs=20,validation_split=0.2,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate((x_test_Article_pad, x_test_Title_pad, x_test_Author_pad),y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "#plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
